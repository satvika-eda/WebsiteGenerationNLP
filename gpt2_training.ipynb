{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a0f588-213d-43ce-aa4f-c81b6aeafbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers[torch]\n",
      "  Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3\n",
      "  Using cached huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 101.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from transformers[torch]) (2021.4.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from transformers[torch]) (5.4.1)\n",
      "Requirement already satisfied: requests in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from transformers[torch]) (2.25.1)\n",
      "Requirement already satisfied: filelock in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from transformers[torch]) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from transformers[torch]) (4.59.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from transformers[torch]) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from transformers[torch]) (1.20.1)\n",
      "Collecting accelerate>=0.21.0\n",
      "  Using cached accelerate-0.29.1-py3-none-any.whl (297 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.2.2-cp38-cp38-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[K     |█████████████▏                  | 310.7 MB 159.7 MB/s eta 0:00:03"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████▋   | 676.7 MB 139.6 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 755.5 MB 27 kB/s /s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: psutil in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.8.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from packaging>=20.0->transformers[torch]) (2.4.7)\n",
      "Requirement already satisfied: jinja2 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from torch->transformers[torch]) (2.11.3)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Requirement already satisfied: networkx in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from torch->transformers[torch]) (2.5)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Requirement already satisfied: sympy in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from torch->transformers[torch]) (1.8)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting triton==2.2.0\n",
      "  Downloading triton-2.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 167.9 MB 174.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from jinja2->torch->transformers[torch]) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from networkx->torch->transformers[torch]) (5.0.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests->transformers[torch]) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests->transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests->transformers[torch]) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests->transformers[torch]) (2.10)\n",
      "Requirement already satisfied: mpmath>=0.19 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sympy->torch->transformers[torch]) (1.2.1)\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, typing-extensions, nvidia-cusparse-cu12, nvidia-cublas-cu12, fsspec, triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusolver-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, huggingface-hub, torch, tokenizers, safetensors, transformers, accelerate\n",
      "Successfully installed accelerate-0.29.1 fsspec-2024.3.1 huggingface-hub-0.22.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 safetensors-0.4.2 tokenizers-0.15.2 torch-2.2.2 transformers-4.39.3 triton-2.2.0 typing-extensions-4.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c67ce53-9fc3-43e5-b888-b4b0589962bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eda8039-b582-48f8-9a14-3daf6807409b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is installed. Version: 2.2.2+cu121\n"
     ]
    }
   ],
   "source": [
    "if torch.__version__:\n",
    "    print(\"PyTorch is installed. Version:\", torch.__version__)\n",
    "else:\n",
    "    print(\"PyTorch is not installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90d7ed83-0797-4b55-8e3e-1e03d76cab15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "Tensor device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set device to GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    # Set device to CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, falling back to CPU\")\n",
    "\n",
    "# Example tensor creation\n",
    "tensor = torch.randn(3, 3).to(device)\n",
    "\n",
    "# Check which device the tensor is on\n",
    "print(\"Tensor device:\", tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e25be34-d730-4a05-8a21-129ca6b86d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bc9a4b2-036f-4be8-a6ce-450bd90af033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA (GPU acceleration) is available, and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21938dbd-d297-4d63-83ed-e23aca785d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer and model imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import the GPT2Tokenizer and GPT2LMHeadModel from the \"gpt2\" pre-trained model\n",
    "try:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    print(\"Tokenizer and model imported successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "# Set the padding token of the tokenizer to the end-of-sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00d88cef-1c3f-4256-a7de-6ba08d62db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HTMLDataset(Dataset):\n",
    "    def __init__(self, prompts, html_outputs, max_length=1024):\n",
    "        \"\"\"\n",
    "        Initializes the HTMLDataset class.\n",
    "\n",
    "        Args:\n",
    "        - prompts (list): List of prompts (input sequences).\n",
    "        - html_outputs (list): List of HTML outputs (target sequences).\n",
    "        - max_length (int): Maximum length of each sequence chunk.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.prompts = prompts\n",
    "        self.html_chunks = []\n",
    "        self.prompt_chunk_mapping = []\n",
    "        self.max_length = max_length\n",
    "\n",
    "        for i, html_output in enumerate(html_outputs):\n",
    "            # Split each HTML output into chunks of maximum length `max_length`\n",
    "            chunks = [html_output[j:j+max_length] for j in range(0, len(html_output), max_length)]\n",
    "            self.html_chunks.extend(chunks)\n",
    "            \n",
    "            # Store the index of the prompt corresponding to each chunk\n",
    "            self.prompt_chunk_mapping.extend([i] * len(chunks))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "\n",
    "        Args:\n",
    "        None\n",
    "\n",
    "        Returns:\n",
    "        int: Total number of samples\n",
    "        \"\"\"\n",
    "        return len(self.html_chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a prompt and its corresponding HTML chunk for the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the prompt tensor and the HTML chunk tensor.\n",
    "        \"\"\"\n",
    "        prompt_idx = self.prompt_chunk_mapping[idx]\n",
    "        prompt = tokenizer.encode(self.prompts[prompt_idx], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=1024)\n",
    "        html_chunk = tokenizer.encode(self.html_chunks[idx], return_tensors=\"pt\", padding=\"max_length\", max_length=self.max_length)\n",
    "        return prompt, html_chunk\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Collates a batch of data samples into a format suitable for the model.\n",
    "\n",
    "        Args:\n",
    "        - batch (list): List of samples, where each sample is a tuple containing the prompt and HTML chunk.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing the padded prompts and HTML chunks.\n",
    "        \"\"\"\n",
    "        prompts, html_outputs = zip(*batch)\n",
    "\n",
    "        prompts = pad_sequence(prompts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "        html_outputs = pad_sequence(html_outputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "        return prompts, html_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "756c8980-5f8c-4b32-aaf5-2b94d039f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [] # List of prompts\n",
    "html_outputs = [] # List of corresponding HTML outputs\n",
    "# dataset = HTMLDataset(prompts, html_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63fb5ef0-fa35-402a-aaba-6b4a8cab7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'files/105000.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d3bc9c8-2863-41a6-802b-ac5a2be65b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Opening the json file and read content using pandas.\n",
    "    Get 'prompt' and 'output' columns from the DataFrame and update them in Python lists.\n",
    "'''\n",
    "with open(file1, 'r') as f:\n",
    "  data = pd.read_json(f)\n",
    "  prompts = data['prompt'].tolist()\n",
    "  html_outputs = data['output'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77d9800d-73aa-40ce-ac08-0512c7484a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'create a Arts website for Rick Astley Headquarters Intro'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[len(prompts)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "198e7204-bdd1-486f-93b6-0869edd84d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53434ed4-4c36-4629-b005-fb4a39dfbbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limiting the number of prompts to 500 for model fine tuning by slicing the 'prompts' list\n",
    "prompts = prompts[:500]\n",
    "html_outputs = html_outputs[:500]\n",
    "len(html_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "821f9cba-d7b6-4412-ae1a-d7a77d35498d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1563941"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the length of the longest HTML output in the 'html_outputs' list\n",
    "len(max(html_outputs, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6234809-4178-49c4-9c12-b0552751bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a HTMLDataset object by passing prompts and HTML outputs as inputs\n",
    "dataset = HTMLDataset(prompts, html_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed4919a2-be3e-46c5-b2d3-26f5138851ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The batch size for the dataloader\n",
    "batch_size = 3\n",
    "\n",
    "'''Create a DataLoader object to efficiently load data in batches:\n",
    " - `dataset`: The dataset to load batches from\n",
    " - `batch_size`: The number of samples to include in each batch\n",
    " - `shuffle=True`: Shuffle the dataset before creating batches to improve randomness and prevent model overfitting\n",
    " - `collate_fn=dataset.collate_fn`: Specifies a function to collate (combine) individual data samples into batches.\n",
    "   In this case, `dataset.collate_fn` is a custom function defined in the HTMLDataset class to pad sequences in batches.\n",
    "'''\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=dataset.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67c2d1d1-fcac-4620-be50-eca9db935d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.HTMLDataset"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c8b0308-25d0-4a93-b6ab-f2d992657cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs for training\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "729ed400-513f-4e6e-96f0-aed4ab6b1964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khatri.say/.local/lib/python3.8/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining the optimizer for updating the model parameters using the AdamW optimizer with a learning rate of 0.01.\n",
    "optimizer = AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "# Calculating the total number of training steps by multiplying the number of batches (length of the dataloader) by the number of epochs.\n",
    "total_steps = len(dataloader) * num_epochs\n",
    "\n",
    "'''\n",
    "    Defining the learning rate scheduler with warm-up for controlling the learning rate during training.\n",
    "    It gradually increases the learning rate during the warm-up phase and then decreases it linearly for the remaining training steps.\n",
    "'''\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c54c0fc-8fe2-4e37-bd7c-74d0628d5b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ac0a083e8104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m                             )\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    511\u001b[0m                 \u001b[0;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Moving the model to the specified device (CPU or GPU) for training and set the model in training mode to enable gradients computation and parameter updates.\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Iterating through each epoch in the specified number of epochs.\n",
    "for epoch in range(num_epochs):\n",
    "    print('epoch: ', epoch)\n",
    "    count = 0 # Increment the batch counter\n",
    "    \n",
    "    # Iterate through each batch in the dataloader.\n",
    "    for batch in dataloader:\n",
    "        count += 1\n",
    "        \n",
    "        # Unpack the batch into prompts and HTML outputs\n",
    "        prompts, html_outputs = batch\n",
    "        \n",
    "        # Move the prompts and HTML outputs to the specified device\n",
    "        prompts = prompts.to(device)\n",
    "        html_outputs = html_outputs.to(device)\n",
    "        \n",
    "        # Pass the prompts and labels through the model to get the outputs\n",
    "        outputs = model(prompts, labels=html_outputs)\n",
    "        \n",
    "        # Get the loss from the model outputs\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Perform backward pass to compute gradients.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update model parameters using the optimizer.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Adjust the learning rate scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Clear gradients after updating parameters.\n",
    "        optimizer.zero_grad()\n",
    "    print(count)\n",
    "# Save the fine-tuned model after training \n",
    "model.save_pretrained(\"gpt2-html-generator\")\n",
    "\n",
    "# Save the entire model (including optimizer and scheduler states) for future use.\n",
    "torch.save(model, 'entire_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e4bae5f6-75e0-4494-a3ab-6a0d5e59b17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 3            |        cudaMalloc retries: 4         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  31663 MiB |  31864 MiB | 106352 MiB |  74688 MiB |\\n|       from large pool |  31648 MiB |  31849 MiB | 106137 MiB |  74488 MiB |\\n|       from small pool |     15 MiB |     16 MiB |    215 MiB |    200 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  31663 MiB |  31864 MiB | 106352 MiB |  74688 MiB |\\n|       from large pool |  31648 MiB |  31849 MiB | 106137 MiB |  74488 MiB |\\n|       from small pool |     15 MiB |     16 MiB |    215 MiB |    200 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |  31652 MiB |  31862 MiB | 106339 MiB |  74687 MiB |\\n|       from large pool |  31637 MiB |  31847 MiB | 106124 MiB |  74486 MiB |\\n|       from small pool |     15 MiB |     16 MiB |    215 MiB |    200 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  32088 MiB |  32090 MiB |  36700 MiB |   4612 MiB |\\n|       from large pool |  32072 MiB |  32072 MiB |  36680 MiB |   4608 MiB |\\n|       from small pool |     16 MiB |     18 MiB |     20 MiB |      4 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory | 434188 KiB |   1877 MiB |  43857 MiB |  43433 MiB |\\n|       from large pool | 433231 KiB |   1876 MiB |  43647 MiB |  43224 MiB |\\n|       from small pool |    957 KiB |      2 MiB |    210 MiB |    209 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |    1117    |    1119    |    2274    |    1157    |\\n|       from large pool |     787    |     789    |    1631    |     844    |\\n|       from small pool |     330    |     331    |     643    |     313    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |    1117    |    1119    |    2274    |    1157    |\\n|       from large pool |     787    |     789    |    1631    |     844    |\\n|       from small pool |     330    |     331    |     643    |     313    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     228    |     229    |     242    |      14    |\\n|       from large pool |     220    |     220    |     232    |      12    |\\n|       from small pool |       8    |       9    |      10    |       2    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |     118    |     122    |     631    |     513    |\\n|       from large pool |     116    |     120    |     498    |     382    |\\n|       from small pool |       2    |       6    |     133    |     131    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5406b269-4392-404c-87ba-90273ab4133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
