{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd82699d-b62a-435b-a4b8-11fe92893adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers bitsandbytes accelerate  # we need latest transformers for this\n",
    "!pip install peft==0.5.0\n",
    "!pip install -U datasets\n",
    "import locale # colab workaround\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\" # colab workaround\n",
    "!pip install wandb\n",
    "!pip install torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575de79d-961a-48e6-ab1d-93c5a52ba833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5544d10-cf50-4324-9b7e-f0dc7376c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b2c7fb-955a-467a-ad2e-9d4ea360c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the notebook_login function from the huggingface_hub module\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# This function call will prompt for your Hugging Face API token and log you into the Hugging Face Hub\n",
    "# directly from a Jupyter notebook. It's useful for accessing private models or datasets and managing API calls.\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917de681-dcb7-45e7-89d7-88614b936bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the load_dataset function from the 'datasets' module, which is part of the Hugging Face 'datasets' library\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset named \"ttbui/html_alpaca\" from the Hugging Face dataset repository, specifying that\n",
    "# we only want the 'train' split of this dataset.\n",
    "dataset = load_dataset(\"ttbui/html_alpaca\", split=\"train\")\n",
    "\n",
    "# Split the loaded training dataset into training and test datasets using the train_test_split function.\n",
    "# Here, 10% of the data is reserved for the test set. The \"train\" subset of this split is assigned to train_dataset.\n",
    "train_dataset = dataset.train_test_split(test_size=0.1)[\"train\"]\n",
    "\n",
    "# Similarly, this line retrieves the \"test\" subset from the same split, assigning it to eval_dataset.\n",
    "eval_dataset = dataset.train_test_split(test_size=0.1)[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2589cf-b624-436e-859a-921e357c42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec012689-811a-4db9-9020-e00eb22fb60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the identifier for the pretrained model that you want to use. In this case, the model is\n",
    "# \"codellama/CodeLlama-7b-hf\" from the Hugging Face model hub.\n",
    "base_model = \"codellama/CodeLlama-7b-hf\"\n",
    "\n",
    "# Load the pretrained model specified by `base_model` using the AutoModelForCausalLM class,\n",
    "# which is designed for causal language modeling tasks (like generating text based on previous context).\n",
    "# The model is configured to load in 8-bit format for memory efficiency, use 16-bit floating point precision,\n",
    "# and automatically map the model to the available device (GPU/CPU) for optimal performance.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,  # Enables 8-bit quantization to reduce memory usage\n",
    "    torch_dtype=torch.float16,  # Sets the data type to 16-bit floating point to save memory and potentially speed up computation\n",
    "    device_map=\"auto\",  # Automatically places model layers on the most appropriate device (CPU/GPU)\n",
    ")\n",
    "\n",
    "# Load the tokenizer associated with the specified model. Tokenizers convert input text into a format\n",
    "# that the model can understand (i.e., convert strings to token ids).\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456492db-f1ce-4cfe-92e2-35696f45abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61452f62-46df-4dc6-95b0-c74b6858e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation prompt for the model. This prompt sets the context for the task the model needs to perform:\n",
    "# generating HTML code for a health website focused on common behavior disorders in children.\n",
    "eval_prompt = \"\"\"You are a powerful text-to-HTML-generation model. Your job is to generate code. You are given a question.\n",
    "\n",
    "You must output the HTML code that answers the question.\n",
    "### Input:\n",
    "create a Health website for 4 Common Behavior Disorders in Children\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "# Use the tokenizer to convert the eval_prompt into a format suitable for the model (token IDs). \n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Put the model in evaluation mode. \n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculations to save memory and speed up computation since gradients are not needed during evaluation.\n",
    "with torch.no_grad():\n",
    "    # Generate text based on the model_input. The model generates tokens until it reaches 500 new tokens,\n",
    "    # and then stops to form the complete output text.\n",
    "    # The output tokens are then decoded back into string format, omitting any special tokens like padding or EOS.\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=500)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0385e1b-fe0b-48ac-90fd-d34a83ee8906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the `add_eos_token` attribute of the tokenizer to True. This ensures that an end-of-sequence (EOS)\n",
    "# token is automatically appended to each sequence processed by the tokenizer. \n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "# Set the `pad_token_id` attribute of the tokenizer to 0. This is the token ID that the tokenizer will\n",
    "# use to pad sequences to a uniform length when batching together multiple sequences.\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "# Set the `padding_side` attribute of the tokenizer to \"left\". This indicates that padding, if necessary,\n",
    "# should be added to the left side of the sequence rather than the right.\n",
    "tokenizer.padding_side = \"left\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b666138d-0fe2-41db-8037-e5d796de3745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function named 'tokenize' that takes a single parameter 'prompt'.\n",
    "# This function processes the prompt to prepare it for model input.\n",
    "def tokenize(prompt):\n",
    "    # Tokenize the input prompt using the global tokenizer object. The tokenizer converts the prompt into\n",
    "    # token IDs with the following settings:\n",
    "    # - Truncation: Long inputs are cut down to the maximum length.\n",
    "    # - max_length: Set the maximum number of tokens in the sequence to 512.\n",
    "    # - padding: No padding is added to the sequences; each sequence retains its original length.\n",
    "    # - return_tensors: Do not convert the output to tensor format (e.g., PyTorch tensors).\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # Create a new key 'labels' in the result dictionary. This is commonly done in self-supervised learning\n",
    "    # tasks where the input itself serves as the label for training. Here, the 'labels' are a copy of the 'input_ids',\n",
    "    # which are the token IDs corresponding to the input prompt.\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    # Return the modified dictionary which now includes both token IDs and labels.\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7a3cf-c124-41a6-b440-ee18f798b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function named 'generate_and_tokenize_prompt' that takes one parameter 'data_point'.\n",
    "# This function generates a formatted prompt for a text-to-HTML generation task and then tokenizes it.\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    # Construct a full prompt using an f-string for string interpolation. The prompt sets the context\n",
    "    # as a powerful text-to-HTML generation model tasked with generating HTML code in response to a question.\n",
    "    # The actual content of the question ('instruction') and the expected output HTML code ('output') are\n",
    "    # dynamically inserted from the 'data_point' dictionary.\n",
    "    full_prompt = f\"\"\"You are a powerful text-to-HTML-generation model. Your job is to generate code. You are given a question.\n",
    "\n",
    "You must output the HTML code that answers the question.\n",
    "\n",
    "### Input:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"output\"]}\n",
    "\"\"\"\n",
    "\n",
    "    # Call the 'tokenize' function, defined earlier, passing the fully constructed prompt.\n",
    "    # The 'tokenize' function handles the specifics of converting the string prompt into a format\n",
    "    # suitable for model input, including tokenization and setting up labels for self-supervised learning.\n",
    "    return tokenize(full_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff5afbb-9c9d-4a3e-9fb8-f568d1ccfa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the 'generate_and_tokenize_prompt' function to each example in the train_dataset.\n",
    "# The 'map' method iteratively applies the function to each element of the dataset.\n",
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "\n",
    "# Similarly, apply the 'generate_and_tokenize_prompt' function to each example in the eval_dataset.\n",
    "# This ensures that the validation dataset is processed in the same way as the training dataset,\n",
    "# allowing for consistent model evaluation.\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d7a11-1a6a-4ee4-83d1-8a7a2d8d0e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained model and tokenizer from Hugging Face's model hub.\n",
    "base_model = \"codellama/CodeLlama-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,  # Reduce model size for faster loading.\n",
    "    torch_dtype=torch.float16,  # Use 16-bit floats to decrease memory usage.\n",
    "    device_map=\"auto\",  # Automatically distribute the model across available devices.\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Add a padding token to the tokenizer if it doesn't already have one, and adjust the model's token embeddings accordingly.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))  # Update the model to handle the new number of tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c447cfd1-84fc-45f9-bfee-9874470628e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switches the model to training mode, enabling layers like dropout and batch normalization.\n",
    "model.train()\n",
    "\n",
    "# Prepares the model for INT8 training for improved efficiency and reduced memory usage.\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# Configure LoRA (Low-Rank Adaptation) parameters for enhancing the model with minimal extra parameters.\n",
    "config = LoraConfig(\n",
    "    r=16,  # Rank of the adaptation.\n",
    "    lora_alpha=16,  # Scaling factor for LoRA weights.\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Transformer modules to apply LoRA.\n",
    "    lora_dropout=0.05,  # Dropout rate for LoRA adaptations.\n",
    "    bias=\"none\",  # Specifies the bias configuration in LoRA layers.\n",
    "    task_type=\"CAUSAL_LM\",  # Type of task for which the model is being adapted.\n",
    ")\n",
    "\n",
    "# Apply the LoRA configuration to the model for efficient parameter-efficient fine-tuning.\n",
    "model = get_peft_model(model, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe99ea-f7ed-419a-ba38-86184794eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the Weights & Biases project for tracking and organizing experiments.\n",
    "wandb_project = \"html-generator\"\n",
    "\n",
    "# Check if the project name string is not empty.\n",
    "if len(wandb_project) > 0:\n",
    "    # Set an environment variable 'WANDB_PROJECT' to the name of the Weights & Biases project.\n",
    "    # This environment variable is used by the wandb library to associate the running script with a specific project.\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ec6b9-4a79-45a9-a87a-3b2ae69df8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if more than one GPU is available in the system using PyTorch.\n",
    "if torch.cuda.device_count() > 1:\n",
    "    # Set the 'is_parallelizable' attribute of the model to True. This indicates that the model\n",
    "    # can be parallelized across multiple GPUs, avoiding automatic data parallelism attempts by some frameworks.\n",
    "    model.is_parallelizable = True\n",
    "\n",
    "    # Enable model parallelism. This setting directs the framework to split the model across\n",
    "    # multiple GPUs, which can lead to more efficient utilization of multiple GPUs during training or inference.\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf29b0-4471-425e-91b0-49772033f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dccea9-93a2-4c2b-a306-9fa8d371cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e1a43a-33c3-4319-b0ae-ac06cbe0c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch sizes and calculate how many gradient accumulation steps are needed\n",
    "# based on the per-device batch size.\n",
    "batch_size = 128\n",
    "per_device_train_batch_size = 32\n",
    "gradient_accumulation_steps = batch_size // per_device_train_batch_size\n",
    "\n",
    "# Set the directory where training outputs will be saved.\n",
    "output_dir = \"html-code-llama\"\n",
    "\n",
    "# Configure training parameters. This setup includes the batch size per device, number of gradient accumulation steps,\n",
    "# learning rate settings, and logging frequency. Evaluation and saving strategies are also set to occur at specified step intervals.\n",
    "training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_steps=100,  # Number of steps to perform learning rate warmup.\n",
    "        max_steps=400,  # Total number of training steps.\n",
    "        learning_rate=3e-4,  # Starting learning rate.\n",
    "        fp16=True,  # Use mixed precision training.\n",
    "        logging_steps=10,  # Interval of training steps between logging.\n",
    "        optim=\"adamw_torch\",  # Use the AdamW optimizer from the Torch library.\n",
    "        evaluation_strategy=\"steps\",  # Perform evaluation at specified intervals.\n",
    "        save_strategy=\"steps\",  # Save the model at specified intervals.\n",
    "        eval_steps=20,  # Number of steps between evaluations.\n",
    "        save_steps=20,  # Number of steps between model saves.\n",
    "        output_dir=output_dir,  # Directory for saving output files.\n",
    "        group_by_length=True,  # Improve efficiency by grouping sequences of similar lengths.\n",
    "        report_to=\"wandb\",  # Send all logging output to Weights & Biases.\n",
    "        run_name=f\"codellama-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\",  # Unique run name based on the current date and time.\n",
    "    )\n",
    "\n",
    "# Initialize the Trainer, which handles the training and evaluation loops.\n",
    "# It is configured with the model, datasets, training arguments, and data collator which handles how data batches are formed.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b18a1-5bf6-4685-a7b1-1c913c10cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable caching within the model's configuration to potentially increase efficiency or handle specific use cases\n",
    "# where caching previous states is not desirable.\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Backup the original state dictionary method of the model.\n",
    "old_state_dict = model.state_dict\n",
    "\n",
    "# Override the state dictionary method to use a custom function that integrates Low-Rank Adaptation (LoRA)\n",
    "# adjustments before returning the state. This is done via a lambda that captures the old state dictionary method.\n",
    "model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(\n",
    "    model, type(model)\n",
    ")\n",
    "\n",
    "# Conditional compilation of the model based on the Python environment:\n",
    "# Only compile if using PyTorch version 2 or higher and not on a Windows platform.\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    print(\"compiling the model\")\n",
    "    # Use PyTorch's just-in-time compilation to optimize the model for the hardware it's running on.\n",
    "    model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa03606-9d65-449a-b142-5962f854c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training the model using the Trainer object. \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0873efe-d776-42b3-a8cf-9fdcfa235939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PyTorch library and specific classes from the Hugging Face Transformers library.\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define the base model identifier for loading the model from Hugging Face's Model Hub.\n",
    "base_model = \"codellama/CodeLlama-7b-hf\"\n",
    "\n",
    "# Load the pre-trained causal language model with configurations optimized for lower memory usage.\n",
    "# This includes using 8-bit quantization and 16-bit floating point precision, and automatically mapping the model to the best available device.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,  # Enable 8-bit model loading for reduced model size.\n",
    "    torch_dtype=torch.float16,  # Use 16-bit floating point types for tensors.\n",
    "    device_map=\"auto\",  # Automatically map model layers to available devices (GPU/CPU).\n",
    ")\n",
    "\n",
    "# Load the tokenizer that matches the pre-trained model, which is necessary for processing text inputs.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c96196-0a58-4a17-a4af-2deeb739b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PeftModel class from the peft module.\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load a pre-trained model into a PEFT model wrapper. This wrapper might handle specialized tasks like fine-tuning or\n",
    "# applying low-rank adaptations for more efficient parameter usage. The model is initialized from a previously saved \n",
    "# state or configuration stored under \"model_code_llama\".\n",
    "model = PeftModel.from_pretrained(model, \"model_code_llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef543ad3-4737-404a-acc9-b909229e5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation prompt that instructs the model to generate HTML code for a specific purpose.\n",
    "eval_prompt = \"\"\"You are a powerful text-to-HTML-generation model. Your job is to generate code. You are given a question.\n",
    "\n",
    "You must output the HTML code that answers the question.\n",
    "### Input:\n",
    "Create a Health website for Dalmane (Flurazepam): Side Effects, Uses, Dosage, Interactions, Warnings.\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the prompt text and prepare it for the model by converting it into a format (tensor) suitable for processing,\n",
    "# and ensure it is placed on a CUDA device for GPU acceleration.\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Set the model to evaluation mode, which is crucial for making predictions as it disables certain layers like dropout.\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculations for efficiency, as they are unnecessary during inference.\n",
    "with torch.no_grad():\n",
    "    # Generate text (HTML code) in response to the input prompt, limiting the generation to 500 new tokens.\n",
    "    # Decode the generated tokens back into a string while skipping special tokens like padding or EOS.\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=500)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e88998-c7bc-4d02-b9f8-9573fc9be4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to the directory \"model_code_llama\".\n",
    "trainer.save_model(\"model_code_llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b41b27-daca-4d52-bd9d-0d7dd573a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to the specified directory for later use.\n",
    "model.save_pretrained(\"model_code_llama\")\n",
    "\n",
    "# Save the associated tokenizer to the same directory as the model.\n",
    "tokenizer.save_pretrained(\"model_code_llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be1ec5-acc4-485b-820d-d29014368b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation prompt for the text-to-HTML generation task.\n",
    "eval_prompt = \"\"\"You are a powerful text-to-HTML-generation model. Your job is to generate code. You are given a question.\n",
    "\n",
    "You must output the HTML code that answers the question.\n",
    "### Input:\n",
    "Create a Health website for Self-Improvement Strategies for Mental Health.\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "# Convert the text prompt into a format (tensor) that the model can process and move it to a GPU device for efficient computation.\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Set the model to evaluation mode to disable training-specific operations like dropout.\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculations to enhance performance during the inference process.\n",
    "with torch.no_grad():\n",
    "    # Generate HTML code from the prompt, limit the output to 500 tokens, and decode the result into plain text.\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=500)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1defcde9-d75d-4aaf-b4be-28d304d767eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input prompt that instructs the model to generate HTML for a specific content type.\n",
    "eval_prompt = \"\"\"You are a powerful text-to-HTML-generation model. Your job is to generate code. You are given a question.\n",
    "\n",
    "You must output the HTML code that answers the question.\n",
    "### Input:\n",
    "Create a Health website for Self-Improvement Strategies for Mental Health.\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "# Prepare the model input by tokenizing the prompt and loading it onto the GPU for efficient processing.\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Set the model to evaluation mode, ensuring it's ready for inference without tracking gradients.\n",
    "model.eval()\n",
    "\n",
    "# Execute the model's generation function within a context that prevents PyTorch from calculating gradients,\n",
    "# enhancing performance during the inference phase.\n",
    "with torch.no_grad():\n",
    "    # Generate text based on the input, limit the output to 500 new tokens, and decode the tokens to plain text.\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=500)[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
