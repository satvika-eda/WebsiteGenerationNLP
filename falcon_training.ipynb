{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f754d-bbf8-4788-987a-8ae5a7dd2d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, GenerationConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec07fd95-6a19-41bc-8eb0-56c991c1bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate\n",
    "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "!pip install trl\n",
    "!pip install datasets\n",
    "!pip install numpy==1.22.1\n",
    "!python3 -m pip install --upgrade h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648a8b6-5e63-4232-8abf-4bc71139c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977cb2f3-bd84-4eee-9d49-662d6e2b9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Execute the notebook_login function to start the authentication process.\n",
    "# This will prompt you to enter your Hugging Face API token directly in the notebook,\n",
    "# which enables you to access private models or datasets and manage API usage.\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c594c424-92ee-4893-8030-3c32fb16bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import accelerate\n",
    "import bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f4a8c-e358-4fb2-9db4-787dc418ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration for a parameter-efficient fine-tuning model.\n",
    "config = PeftConfig.from_pretrained(\"PrincySinghal991/falcon-7b-sharded-bf16-finetuned-html-code-generation\")\n",
    "\n",
    "# Define the model name for the pretrained model.\n",
    "model_name = \"ybelkada/falcon-7b-sharded-bf16\"  # sharded falcon-7b model\n",
    "\n",
    "# Configure model loading and precision using BitsAndBytesConfig.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,            # Load model in 4-bit precision for efficiency.\n",
    "    bnb_4bit_quant_type=\"nf4\",    # Quantize pre-trained model in 4-bit NF format.\n",
    "    bnb_4bit_use_double_quant=True, # Use double quantization to enhance precision.\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Use BF16 format during computation for performance.\n",
    ")\n",
    "\n",
    "# Load the model with specific quantization and device placement settings.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config, # Apply quantization settings.\n",
    "    device_map=\"auto\",  # Automatically assign model layers to optimal devices.\n",
    "    trust_remote_code=True, # Allow execution of custom remote code.\n",
    ")\n",
    "\n",
    "# Convert the loaded model to a PEFT model with specific configurations.\n",
    "model = PeftModel.from_pretrained(model, \"PrincySinghal991/falcon-7b-sharded-bf16-finetuned-html-code-generation\",\n",
    "    torch_dtype=torch.float16,  # Set tensor data type to float16 for memory efficiency.\n",
    "    trust_remote_code=True,     # Enable execution of remote code for this model as well.\n",
    "    device_map='auto',          # Auto-assign model components to devices.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d0115b-7e45-45df-911c-75377fce4247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a tokenizer for the specified model, allowing for the execution of remote code and setting the padding direction.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,                 # Model identifier from Hugging Face's model hub.\n",
    "    trust_remote_code=True,     # Allow execution of custom code associated with the model.\n",
    "    padding_side='left',        # Specify that padding should be added to the left side of token sequences.\n",
    ")\n",
    "\n",
    "# Set the padding token to be the same as the end-of-sequence token for the tokenizer.\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0059f2-d63d-4539-8e21-41830ddd5c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the json module to handle JSON data.\n",
    "import json\n",
    "\n",
    "# Open the file located at \"data/data03.json\" in read mode.\n",
    "f = open(\"data/data03.json\", \"r\")\n",
    "\n",
    "# Load the JSON data from the file into the 'data' variable.\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a367708-6230-44d0-8d6c-11bd0d14d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize two empty lists to store prompts and responses separately.\n",
    "prompts = []\n",
    "responses = []\n",
    "\n",
    "# Loop through each dictionary in the 'data' list loaded from the JSON file.\n",
    "for i in data:\n",
    "    # Append the value associated with the key 'prompt' from each dictionary to the 'prompts' list.\n",
    "    prompts.append(i[\"prompt\"])\n",
    "    # Append the value associated with the key 'output' from each dictionary to the 'responses' list.\n",
    "    responses.append(i[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac64930-a25e-4e8a-bdcf-01b75d8a925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train_test_split function from the sklearn.model_selection module to handle data splitting.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the 'prompts' and 'responses' lists into training and testing sets.\n",
    "# The data is shuffled to ensure randomness and split such that 10% is reserved for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  prompts, responses,      # Lists containing the input data and corresponding labels.\n",
    "  random_state=104,        # Seed for the random number generator for reproducibility.\n",
    "  test_size=0.1,           # Proportion of the dataset to include in the test split.\n",
    "  shuffle=True             # Enable shuffling to randomize the distribution of data between splits.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d223533-3cf2-41bb-9aed-ba7f69586627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DatasetDict and Dataset classes from the datasets library.\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create dictionaries for training and testing datasets using the previously split data.\n",
    "# Each dictionary maps column names to the respective data lists.\n",
    "d = {\n",
    "    'train': Dataset.from_dict({'Code': y_train, 'Prompt': X_train}),  # Create a dataset from the training data.\n",
    "    'test': Dataset.from_dict({'Code': y_test, 'Prompt': X_test})      # Create a dataset from the testing data.\n",
    "}\n",
    "\n",
    "# Combine these datasets into a DatasetDict, which conveniently manages multiple sets (e.g., train and test).\n",
    "webdataset = DatasetDict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec12c34d-314c-48a0-8be0-7ba9820b642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the BitsAndBytesConfig class from the transformers library (unused in the shown code but may be required elsewhere).\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Prepare the model for training using lower bit precision to reduce memory usage and possibly increase speed.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Set LoRA configuration parameters: scaling factor, dropout probability, and rank of the low-rank matrices.\n",
    "lora_alpha = 32  # Scaling factor influencing the magnitude of parameter updates.\n",
    "lora_dropout = 0.05  # Dropout rate for LoRA layers to prevent overfitting.\n",
    "lora_rank = 32  # Rank for the low-rank approximation, balancing model complexity and expressiveness.\n",
    "\n",
    "# Configure LoRA settings for the model, specifying layers to target and other options.\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_rank,\n",
    "    bias=\"none\",  # Opt not to train bias parameters to focus on weights.\n",
    "    task_type=\"CAUSAL_LM\",  # Specify the type of model task.\n",
    "    target_modules=[\n",
    "        \"query_key_value\",  # Transformer sub-layers to apply LoRA.\n",
    "        \"dense\",\n",
    "        \"dense_h_to_4h\",\n",
    "        \"dense_4h_to_h\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the LoRA configuration to the model to create a parameter-efficient fine-tuned (PEFT) model.\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21298cf-87f9-42f9-a226-7222a05eeb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"webcraft_falcon\"\n",
    "per_device_train_batch_size = 2 #batch size of 16 and 32 might give better results\n",
    "gradient_accumulation_steps = 2  # increase gradient accumulation steps by 2x if batch size is reduced ie here Gradients are accumulated for 2 steps before performing a backward pass and updating the model weights.\n",
    "optim = \"paged_adamw_32bit\" # activates the paging for better memory management --> AdamW optimizer variant\n",
    "save_strategy=\"steps\" # checkpoint save strategy to adopt during training\n",
    "save_steps = 20 # number of updates steps before two checkpoint saves\n",
    "logging_steps = 20  # number of update steps between two logs if logging_strategy=\"steps\"\n",
    "learning_rate = 2e-4  # learning rate for AdamW optimizer\n",
    "max_grad_norm = 0.3 # maximum gradient norm (for gradient clipping) to avoid exploding gradient problem\n",
    "max_steps = 320        # training will happen for 320 steps\n",
    "warmup_ratio = 0.03 # number of steps used for a linear warmup from 0 to learning_rate--> Warmup slowly increases the learning rate from 0 to the specified learning rate to help stabilize training in the early stages.\n",
    "lr_scheduler_type = \"cosine\"  # learning rate follows cosine curve--> slowly decrease lr\n",
    "\n",
    "# Configure training parameters to fine-tune the model with specific hardware optimization and learning strategies.\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"webcraft_falcon\",  # Directory for saving training outputs.\n",
    "    per_device_train_batch_size=2,  # Small batch size could be increased for better performance.\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps to effectively increase batch size.\n",
    "    optim=\"paged_adamw_32bit\",  # Use a memory-efficient variant of the AdamW optimizer.\n",
    "    save_steps=20,  # Save model checkpoint every 20 steps.\n",
    "    logging_steps=20,  # Log training progress every 20 steps.\n",
    "    learning_rate=2e-4,  # Set learning rate for the optimizer.\n",
    "    bf16=False,  # Do not use Brain Floating Point 16-bit precision.\n",
    "    max_grad_norm=0.3,  # Clip gradients to a maximum norm of 0.3 to prevent gradient explosion.\n",
    "    max_steps=320,  # Set a finite number of training steps to 320.\n",
    "    warmup_ratio=0.03,  # Linear warmup over 3% of training steps.\n",
    "    group_by_length=True,  # Group similar length prompts to improve batching efficiency.\n",
    "    lr_scheduler_type=\"cosine\",  # Use a cosine learning rate scheduler.\n",
    "    push_to_hub=True,  # Automatically push checkpoints to the Hugging Face Hub.\n",
    "    tf32=False,  # Disable TensorFlow 32-bit precision.\n",
    "    evaluation_strategy=\"steps\",  # Evaluate model periodically during training.\n",
    "    eval_steps=20,  # Perform evaluation every 20 steps.\n",
    "    load_best_model_at_end=True,  # Load the best performing model at the end of training.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d022e-e9de-4754-8188-2f9d3122a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the EarlyStoppingCallback from the transformers library, which can be used to halt training\n",
    "# when a monitored metric has stopped improving.\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Open the file \"data/data27.json\" in read mode to access its contents.\n",
    "f2 = open(\"data/data27.json\", \"r\")\n",
    "\n",
    "# Load the JSON content from the opened file into the 'test_data' variable. This JSON data is typically \n",
    "# structured as a dictionary or list of dictionaries, depending on its layout.\n",
    "test_data = json.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f642e59-cdcb-4492-b75c-4ca79e191d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function named concat_fields that takes an example (dictionary) as input and returns a new dictionary\n",
    "# containing only the 'Code' field from the input example.\n",
    "def concat_fields(example):\n",
    "    return {'Code': example['Code']}\n",
    "\n",
    "# Apply the concat_fields function to each example in the train dataset, retaining only the 'Code' field.\n",
    "train_dataset = webdataset[\"train\"].map(concat_fields)\n",
    "\n",
    "# Apply the concat_fields function to each example in the test dataset, retaining only the 'Code' field.\n",
    "test_dataset = webdataset[\"test\"].map(concat_fields)\n",
    "\n",
    "# Print the resulting train and test datasets to inspect their structure and content.\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1921d96-64ec-4170-91ff-cf1cf894fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SFTTrainer instance with the following parameters:\n",
    "# - peft_model: The parameter-efficient fine-tuned (PEFT) model to be trained.\n",
    "# - train_dataset: The dataset used for training the model.\n",
    "# - peft_config: Configuration for the PEFT model, including LoRA settings.\n",
    "# - dataset_text_field: The field in the dataset containing text data ('Code' in this case).\n",
    "# - max_seq_length: Maximum sequence length for tokenized inputs.\n",
    "# - tokenizer: The tokenizer used to preprocess text inputs.\n",
    "# - args: Training arguments, including optimization settings and training schedule.\n",
    "# - eval_dataset: The dataset used for evaluation during training (test dataset).\n",
    "# - callbacks: Optional list of callbacks, such as early stopping criteria.\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=webdataset[\"train\"],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"Code\",  # Field containing text data in the dataset.\n",
    "    max_seq_length=1024,  # Maximum sequence length for tokenized inputs.\n",
    "    tokenizer=tokenizer,  # Tokenizer used for preprocessing.\n",
    "    args=training_arguments,  # Training arguments and configuration.\n",
    "    eval_dataset=webdataset[\"test\"],  # Evaluation dataset (test dataset).\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Callback for early stopping.\n",
    ")\n",
    "\n",
    "# Adjust the norm layers in the model for dealing with large models with reduced precision.\n",
    "# This loop iterates through all named modules in the model and converts them to float32 if \"norm\" is in their name.\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a0b56-a6f5-4934-9ef2-ab4f8d8be5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable cache usage in the PEFT model configuration to prevent caching during training.\n",
    "peft_model.config.use_cache = False\n",
    "\n",
    "# Initiate the training process by calling the train() method on the trainer object.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff908c2-6662-4970-b2ca-9957144df65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss :\n",
    "        \n",
    "    \n",
    "# [260/320 1:34:09 < 21:53, 0.05 it/s, Epoch 2/4]\n",
    "# Step\tTraining Loss\tValidation Loss\n",
    "# 20\t1.140900\t1.104002\n",
    "# 40\t1.069200\t0.946198\n",
    "# 60\t0.968000\t0.840632\n",
    "# 80\t0.659900\t0.771222\n",
    "# 100\t0.796700\t0.767768\n",
    "# 120\t0.599200\t0.737263\n",
    "# 140\t0.542400\t0.724135\n",
    "# 160\t0.507300\t0.699540\n",
    "# 180\t0.578600\t0.697789\n",
    "# 200\t0.606400\t0.695403\n",
    "# 220\t0.463400\t0.707009\n",
    "# 240\t0.473700\t0.701999\n",
    "# 260\t0.496700\t0.702401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4022f6d4-cfc0-4cf9-9998-c06bc2639789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the trained model checkpoints and associated files to the Hugging Face Model Hub.\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24311b-dd34-4086-bb07-9be54e3678fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target device for inference (GPU).\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "# Tokenize the input text using the tokenizer, returning PyTorch tensors.\n",
    "inputs = tokenizer(\"create a Recreation website for Home - BMW Riders of Oregon\", return_tensors=\"pt\")\n",
    "\n",
    "# Extract input_ids and attention_mask tensors from the tokenized inputs.\n",
    "input_ids = inputs[\"input_ids\"]               # Tokenized input IDs.\n",
    "input_attention_mask = inputs[\"attention_mask\"]  # Attention mask indicating valid tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac0832a-4229-494d-b378-1044b4bed284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute model generation within a no-gradient context to prevent gradient computation and storage.\n",
    "with torch.no_grad():\n",
    "    # Generate output text using the model's generate method.\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,                       # Input token IDs.\n",
    "        attention_mask=input_attention_mask,       # Attention mask for valid tokens.\n",
    "        return_dict_in_generate=True,             # Return a dictionary in the generation output.\n",
    "        max_new_tokens=1000,                       # Maximum number of tokens to generate.\n",
    "        eos_token_id=tokenizer.eos_token_id,       # ID of the end-of-sequence token.\n",
    "    )\n",
    "\n",
    "# Extract the generated sequence from the generation output.\n",
    "generation_output = generation_output.sequences[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f6004-390f-4a16-b2fc-f316421567d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the generated token sequence into human-readable text, skipping special tokens.\n",
    "output = tokenizer.decode(generation_output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a233d1ce-6c3c-4797-80bd-8411aad969f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560cd7f-1e18-458f-8cba-518b7624f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target device for inference (GPU).\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "# Tokenize the input text using the tokenizer, returning PyTorch tensors.\n",
    "inputs = tokenizer(\"create a Recreation website for Home - BMW Riders of Oregon\", return_tensors=\"pt\")\n",
    "\n",
    "# Extract input_ids and attention_mask tensors from the tokenized inputs.\n",
    "input_ids = inputs[\"input_ids\"]               # Tokenized input IDs.\n",
    "input_attention_mask = inputs[\"attention_mask\"]  # Attention mask indicating valid tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46181dd8-fc6c-408a-be62-156430ec324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute model generation within a no-gradient context to prevent gradient computation and storage.\n",
    "with torch.no_grad():\n",
    "    # Generate output text using the model's generate method.\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,                       # Input token IDs.\n",
    "        attention_mask=input_attention_mask,       # Attention mask for valid tokens.\n",
    "        return_dict_in_generate=True,             # Return a dictionary in the generation output.\n",
    "        max_new_tokens=1000,                       # Maximum number of tokens to generate.\n",
    "        eos_token_id=tokenizer.eos_token_id,       # ID of the end-of-sequence token.\n",
    "    )\n",
    "\n",
    "# Extract the generated sequence from the generation output.\n",
    "generation_output = generation_output.sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c3d0f1-fe2a-4f17-8bbb-8fab0163ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the generated token sequence into human-readable text, skipping special tokens.\n",
    "output = tokenizer.decode(generation_output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa870af-fc2e-418b-8469-69cae6ac498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
